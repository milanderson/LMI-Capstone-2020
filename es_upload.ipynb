{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Splitting & ElasticSearch Uploads\n",
    "## LMI Capstone Team\n",
    "## Summer Chambers | Steve Morris | Kaleb Shikur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rule Splitting, Uploading to ES\n",
    "'''\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection, ElasticsearchException\n",
    "# from requests_aws4auth import AWS4Auth\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests #gets urls\n",
    "import time\n",
    "# from aws_requests_auth.boto_utils import BotoAWSRequestsAuth\n",
    "\n",
    "host = 'https://search-lmi-capstone-test-bbewbwsli4uchb7r24zujwrqne.us-east-1.es.amazonaws.com'\n",
    "# auth = BotoAWSRequestsAuth(aws_host=host,\n",
    "#                            aws_region='us-east-1',\n",
    "#                            aws_service='es')\n",
    "# awsauth = AWS4Auth(YOUR_ACCESS_KEY, YOUR_SECRET_KEY, REGION, 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dedbbb462bf60acdb6c63621c8bf117c', 'cluster_name': '846033058400:lmi-capstone-test', 'cluster_uuid': 'vhqZptiWQEq0BVgaN6qi8Q', 'version': {'number': '7.8.0', 'build_flavor': 'oss', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2020-10-15T22:09:28.519938Z', 'build_snapshot': False, 'lucene_version': '8.5.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(host, timeout = 45)\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "print(es.info())\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     http_auth=auth,\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "# print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Rule Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Rule/CMS-2018-0101-0001.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headers(rule_url):\n",
    "\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    rulechunks = {}\n",
    "    \n",
    "    startdict = {'a2':['2. proposals for modified participation options under 5-year agreement periods', '3. creating a basic track with glide path to performance-based risk'], \\\n",
    "     'a3':['3. creating a basic track with glide path to performance-based risk', '4. permitting annual participation elections'], \\\n",
    "    'a4b':['b. proposals for permitting election of differing levels of risk within the basic track\\'s glide path', 'c. proposals for permitting annual election of beneficiary assignment methodology'], \\\n",
    "    'a4c':['c. proposals for permitting annual election of beneficiary assignment methodology', '5. determining participation options based on medicare ffs revenue and prior participation '], \\\n",
    "    'a5b':['b. differentiating between low revenue acos and high revenue acos', 'c. determining participation options based on prior participation of aco legal entity and aco participants'], \\\n",
    "    'a5c':['c. determining participation options based on prior participation of aco legal entity and aco participants', 'd. monitoring for financial performance'], \\\n",
    "    'a5d': ['d. monitoring for financial performance', '6. requirements for aco participation in two-sided models'], \\\n",
    "    'a6b': ['b. election of msr/mlr by acos', 'c. aco repayment mechanisms'], \\\n",
    "    'a6c': ['c. aco repayment mechanisms', 'd. advance notice for and payment consequences of termination '], \\\n",
    "    'a6d2': ['(2) proposals for advance notice of voluntary termination', '(3) proposals for payment consequences of termination'], \\\n",
    "    'a6d3': ['(3) proposals for payment consequences of termination', '7. participation options for agreement periods beginning in 2019'], \\\n",
    "    'a7b': ['b. methodology for determining financial and quality performance for the 6-month performance years during 2019', 'c. applicability of program policies to acos participating in a 6-month performance year'], \\\n",
    "    'a7c': ['c. applicability of program policies to acos participating in a 6-month performance year', 'b. fee-for-service benefit enhancements'], \\\n",
    "    'b2a': ['a. shared savings program snf 3-day rule waiver', 'b. billing and payment for telehealth services'], \\\n",
    "    'b2b': ['b. billing and payment for telehealth services', 'c. providing tools to strengthen beneficiary engagement'], \\\n",
    "    'c2': ['2. beneficiary incentives', '3. empowering beneficiary choice'], \\\n",
    "    'c3a': ['3. empowering beneficiary choice', 'b. beneficiary opt-in based assignment methodology'], \\\n",
    "    'c3b': ['b. beneficiary opt-in based assignment methodology', 'd. benchmarking methodology refinements'],  \\\n",
    "    'd2': ['2. risk adjustment methodology for adjusting historical benchmark each performance year', '3. use of regional factors when establishing and resetting acos\\' benchmarks'], \\\n",
    "    'd3b': ['b. proposals to apply regional expenditures in determining the benchmark for an aco\\'s first agreement period', 'c. proposals for modifying the regional adjustment'], \\\n",
    "    'd3c': ['c. proposals for modifying the regional adjustment', 'd. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark'], \\\n",
    "    'd3d': ['d. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark', '4. technical changes to incorporate references to benchmark rebasing policies'], \\\n",
    "    'd4': ['4. technical changes to incorporate references to benchmark rebasing policies', 'e. updating program policies'], \\\n",
    "    'e2': ['2. revisions to policies on voluntary alignment', '3. revisions to the definition of primary care services used in beneficiary assignment'], \\\n",
    "    'e3': ['3. revisions to the definition of primary care services used in beneficiary assignment', '4. extreme and uncontrollable circumstances policies for the shared savings program'], \\\n",
    "    'e4': ['4. extreme and uncontrollable circumstances policies for the shared savings program', '5. program data and quality measures'], \\\n",
    "    'e5':['5. program data and quality measures', '6. promoting interoperability'], \\\n",
    "    'e6':['6. promoting interoperability', '7. coordination of pharmacy care for aco beneficiaries'], \\\n",
    "    'e7':['7. coordination of pharmacy care for aco beneficiaries', 'f. applicability of proposed policies to track 1+ model acos'], \\\n",
    "    'f2':['2. unavailability of application cycles for entry into the track 1+ model in 2019 and 2020', '3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements'], \\\n",
    "    'f3':['3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements', 'g. summary of proposed timing of applicability']}\n",
    "    \n",
    "    for key, value in startdict.items():    \n",
    "       splitlist = splitlist.split(value[0]) #split on start of desired section\n",
    "       split_further = splitlist[1].split(value[1]) #split again on start of undesired section\n",
    "       rulechunks[key] = {\"text\": value[0]+split_further[0]} #choose only first half to upload to dict\n",
    "       splitlist = splitlist[1] #choose second half to prepare for next split\n",
    "    \n",
    "    #print(rulechunks.keys())   \n",
    "    #print(rulechunks[\"f3\"])\n",
    "    \n",
    "    #lengths = [len(chunk) for chunk in rulechunks.values()]\n",
    "    #print(lengths) #characters\n",
    "    \n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_chars(rule_url):\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    #chunk by num chars\n",
    "    chunklist = [(splitlist[i:i+8000]) for i in range(0, len(splitlist), 8000)]\n",
    "    \n",
    "    #make dict\n",
    "    rulechunks = {}\n",
    "    keys = range(len(chunklist))\n",
    "    for i in keys:\n",
    "        rulechunks[i] = {\"text\": chunklist[i]}\n",
    "\n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headchars(rule_url):\n",
    "    chunks = splitRule_headers(rule_url) #call first method\n",
    "    for key, value in chunks.items():\n",
    "        if len(value[\"text\"]) > 50000: #check to see if chunk is really long\n",
    "            smalldict = {}\n",
    "            chunklist = [value[\"text\"][i:i+25000] for i in range(0, len(value[\"text\"]), 25000)] #split chunk\n",
    "            keys = range(len(chunklist))\n",
    "            for i in keys:\n",
    "                smalldict[key+str(i)] = {\"text\": chunklist[i]} #add to sub-dictionary\n",
    "            chunks[key] = smalldict #add to big dictionary\n",
    "        else:\n",
    "            chunks[key] = {key+str(0): value} #add to big dictionary\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Rule Splits to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesplit_toES(rulechunks, es_index, idnum):\n",
    "    for key, value in rulechunks.items():\n",
    "            res = es.index(index=es_index, id=idnum, body={key:value}, doc_type='_doc')\n",
    "            idnum += 1\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(f\"Last id uploaded: {idnum-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruletoES_hybrid(rulechunks, es_index, idnum):\n",
    "    for key, value in rulechunks.items():\n",
    "        for small_key, small_value in value.items():\n",
    "            res = es.index(index=es_index, id=idnum, body={small_key:small_value})\n",
    "            idnum += 1\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(f\"Last id uploaded: {idnum-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesplit_test = splitRule_headers(rule_url)\n",
    "test_upload = rulesplit_toES(rulesplit_test, \"headers_new\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesplit_test2 = splitRule_chars(rule_url)\n",
    "test_upload2 = rulesplit_toES(rulesplit_test2, \"char_8000\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulesplit_test3 = splitRule_headchars(rule_url)\n",
    "test_upload3 = ruletoES_hybrid(rulesplit_test3, \"hybrid_new\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ES Search\n",
    "def ES_search(index, querydict):\n",
    "    search = es.search(index=index, doc_type=\"_doc\", body={\"query\": querydict})\n",
    "    test_list = {}\n",
    "    if search['hits']['hits'] != []:\n",
    "        for h in search['hits']['hits']:\n",
    "            key = list((h['_source'].keys()))\n",
    "            test_list[key[0]]=h['_score']\n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = {\"simple_query_string\": {\"query\": \"urban\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a6c': 0.45770907, 'a4c': 0.45549875, 'b2b': 0.29446962, 'c2': 0.2554381}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers_test_search = ES_search(\"headers_new\", test_query)\n",
    "headers_test_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'64': 0.29684716}, {'70': 0.29528618}, {'68': 0.29148874}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_test_search = ES_search(\"char_8000\", test_query)\n",
    "char_test_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'c20': 0.40307403},\n",
       " {'a4c15': 0.29141787},\n",
       " {'a6c7': 0.29115358},\n",
       " {'a6c8': 0.29011896},\n",
       " {'a4c16': 0.28976172},\n",
       " {'a4c17': 0.289463},\n",
       " {'a6c9': 0.28831965},\n",
       " {'b2b0': 0.1865641}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_test_search = ES_search(\"hybrid_new\", test_query)\n",
    "hybrid_test_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete an index:\n",
    "# es.indices.delete(index='index_name', ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Comment Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Comments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_comments(comment_url):\n",
    "    comment_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Comments/\"\n",
    "    response = requests.get(comment_url)\n",
    "    soup = BeautifulSoup(response.text, 'html')\n",
    "    a_tags = soup.findAll(\"a\")\n",
    "    links = [tag[\"href\"] for tag in a_tags]\n",
    "    txt_links = [link for link in links if '.txt' in link]\n",
    "    comments = {}\n",
    "    for suffix in txt_links:\n",
    "        comments[suffix] = requests.get(comment_url+suffix).text.lower()\n",
    "        #print(f\"scraping comment {suffix}\")\n",
    "    print(f\"scraped {len(txt_links)} comments\")\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\anaconda3\\envs\\python385\\lib\\site-packages\\bs4\\__init__.py:177: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 194 of the file C:\\Users\\steph\\anaconda3\\envs\\python385\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html.parser\")\n",
      "\n",
      "  warnings.warn(self.NO_PARSER_SPECIFIED_WARNING % dict(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped 674 comments\n"
     ]
    }
   ],
   "source": [
    "comments2018 = retrieve_comments(comment_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_lengths = {key: len(value) for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_comment_lengths = dict(sorted(comment_lengths.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_short = {key: len(value) for key, value in comments2018.items() if \"attach\" in value and len(value) < 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing dictionary \n",
    "stripped_comments = {(key[14:20]): value for key, value in comment_lengths.items()}\n",
    "sorted_keys = sorted(list(stripped_comments.keys()))\n",
    "\n",
    "sorted_keys = sorted(list(stripped_comments.keys()))\n",
    "for i in range(len(sorted_keys)-1):\n",
    "    if sorted_keys[i+1][0:4] == sorted_keys[i][0:4]:\n",
    "        stripped_comments[sorted_keys[i+1]] = stripped_comments[sorted_keys[i]] + stripped_comments[sorted_keys[i+1]]\n",
    "        del(stripped_comments[sorted_keys[i]])\n",
    "        \n",
    "stripped_comments\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = {(key[14:20]): value for key, value in comments2018.items()}\n",
    "sorted_keys = sorted(list(comments2018.keys()))\n",
    "\n",
    "# now for each key in the list\n",
    "for i in range(len(comments2018)-1):\n",
    "    \n",
    "    # get key at index i and key at index i+1 and compare them\n",
    "    if sorted_keys[i+1][0:4] == sorted_keys[i][0:4]:\n",
    "        comments2018[sorted_keys[i+1]] = comments2018[sorted_keys[i]] + comments2018[sorted_keys[i+1]]\n",
    "        del(comments2018[sorted_keys[i]])\n",
    "\n",
    "{key: len(value) for key, value in comments2018.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = {key[0:4]:value for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 10, 'successful': 5, 'failed': 0}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index='headers_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment001_query = {\"simple_query_string\": {\"query\": comments2018[\"0002\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a4c': 144.72498,\n",
       " 'a6c': 142.13535,\n",
       " 'a5c': 110.61184,\n",
       " 'c3b': 106.4453,\n",
       " 'a3': 105.05455,\n",
       " 'a5b': 104.90532,\n",
       " 'a2': 103.80479,\n",
       " 'e4': 100.2969,\n",
       " 'a7c': 97.574455,\n",
       " 'a7b': 93.47521}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"headers_new\", test_comment001_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_first3 = {key:value for key,value in comments2018}\n",
    "dict_first3 = {}\n",
    "for key in list(comments2018.keys())[0:3]:\n",
    "    dict_first3[key] = comments2018[key]\n",
    "dict_first3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "maximum clause error at index 0012\n",
      "10\n",
      "maximum clause error at index 0013\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "maximum clause error at index 0081\n",
      "79\n",
      "maximum clause error at index 0082\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "maximum clause error at index 0138\n",
      "136\n",
      "137\n",
      "maximum clause error at index 0140\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "maximum clause error at index 0190\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "maximum clause error at index 0197\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "maximum clause error at index 0203\n",
      "201\n",
      "maximum clause error at index 0204\n",
      "202\n",
      "203\n",
      "204\n",
      "maximum clause error at index 0207\n",
      "205\n",
      "maximum clause error at index 0208\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "maximum clause error at index 0212\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "maximum clause error at index 0234\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "maximum clause error at index 0238\n",
      "236\n",
      "237\n",
      "238\n",
      "maximum clause error at index 0241\n",
      "239\n",
      "240\n",
      "maximum clause error at index 0243\n",
      "241\n",
      "242\n",
      "maximum clause error at index 0245\n",
      "243\n",
      "244\n",
      "maximum clause error at index 0247\n",
      "245\n",
      "maximum clause error at index 0248\n",
      "246\n",
      "247\n",
      "maximum clause error at index 0250\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "maximum clause error at index 0260\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "maximum clause error at index 0268\n",
      "266\n",
      "maximum clause error at index 0269\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "maximum clause error at index 0279\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "maximum clause error at index 0286\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "maximum clause error at index 0295\n",
      "293\n",
      "maximum clause error at index 0296\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "maximum clause error at index 0301\n",
      "299\n",
      "300\n",
      "301\n",
      "maximum clause error at index 0304\n",
      "302\n",
      "303\n",
      "304\n",
      "maximum clause error at index 0307\n",
      "305\n",
      "maximum clause error at index 0308\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "maximum clause error at index 0312\n",
      "310\n",
      "maximum clause error at index 0313\n",
      "311\n",
      "312\n",
      "313\n",
      "maximum clause error at index 0316\n",
      "314\n",
      "maximum clause error at index 0317\n",
      "315\n",
      "maximum clause error at index 0318\n",
      "316\n",
      "317\n",
      "maximum clause error at index 0320\n",
      "318\n",
      "319\n",
      "maximum clause error at index 0322\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "maximum clause error at index 0326\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "maximum clause error at index 0330\n",
      "328\n",
      "329\n",
      "330\n",
      "maximum clause error at index 0333\n",
      "331\n",
      "332\n",
      "maximum clause error at index 0335\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "maximum clause error at index 0341\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "maximum clause error at index 0345\n",
      "343\n",
      "maximum clause error at index 0346\n",
      "344\n",
      "345\n",
      "346\n",
      "maximum clause error at index 0349\n",
      "347\n",
      "maximum clause error at index 0350\n",
      "348\n",
      "349\n",
      "350\n",
      "maximum clause error at index 0353\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "maximum clause error at index 0357\n",
      "355\n",
      "maximum clause error at index 0358\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "maximum clause error at index 0365\n",
      "363\n",
      "maximum clause error at index 0366\n",
      "364\n",
      "maximum clause error at index 0367\n",
      "365\n",
      "maximum clause error at index 0368\n",
      "366\n",
      "maximum clause error at index 0369\n",
      "367\n",
      "maximum clause error at index 0370\n",
      "368\n",
      "369\n",
      "maximum clause error at index 0372\n",
      "370\n",
      "maximum clause error at index 0373\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "maximum clause error at index 0377\n",
      "375\n",
      "maximum clause error at index 0378\n",
      "376\n",
      "maximum clause error at index 0379\n",
      "377\n",
      "378\n",
      "379\n",
      "maximum clause error at index 0382\n",
      "380\n",
      "maximum clause error at index 0383\n",
      "381\n",
      "382\n",
      "383\n",
      "maximum clause error at index 0386\n",
      "384\n",
      "maximum clause error at index 0387\n",
      "385\n",
      "maximum clause error at index 0388\n",
      "386\n",
      "maximum clause error at index 0389\n",
      "387\n",
      "388\n",
      "maximum clause error at index 0391\n",
      "389\n",
      "390\n",
      "maximum clause error at index 0393\n",
      "391\n",
      "392\n",
      "maximum clause error at index 0395\n",
      "393\n",
      "maximum clause error at index 0396\n",
      "394\n",
      "395\n",
      "maximum clause error at index 0398\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "maximum clause error at index 0406\n",
      "404\n",
      "405\n",
      "406\n",
      "maximum clause error at index 0409\n",
      "407\n",
      "408\n",
      "409\n",
      "maximum clause error at index 0412\n",
      "410\n",
      "maximum clause error at index 0413\n",
      "411\n",
      "412\n",
      "maximum clause error at index 0415\n",
      "413\n",
      "maximum clause error at index 0416\n",
      "414\n",
      "maximum clause error at index 0417\n",
      "415\n",
      "maximum clause error at index 0418\n",
      "416\n",
      "417\n",
      "maximum clause error at index 0420\n",
      "418\n",
      "maximum clause error at index 0421\n",
      "419\n",
      "420\n",
      "maximum clause error at index 0423\n",
      "421\n",
      "maximum clause error at index 0424\n",
      "422\n",
      "423\n",
      "424\n",
      "maximum clause error at index 0427\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "maximum clause error at index 0433\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "maximum clause error at index 0440\n",
      "438\n",
      "maximum clause error at index 0441\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "maximum clause error at index 0445\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "maximum clause error at index 0449\n",
      "447\n",
      "maximum clause error at index 0450\n",
      "448\n",
      "449\n",
      "maximum clause error at index 0452\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "maximum clause error at index 0459\n",
      "457\n",
      "maximum clause error at index 0460\n",
      "458\n",
      "459\n",
      "maximum clause error at index 0462\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "maximum clause error at index 0469\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "# Full dictionary \n",
    "\n",
    "results = {}\n",
    "i=0\n",
    "for key, value in comments2018.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    try:\n",
    "        search1 = ES_search(\"headers_new\", query)\n",
    "    except ElasticsearchException as es1:\n",
    "            print(f'maximum clause error at index {key}')\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(1)\n",
    "    es.indices.refresh(index='headers_new')\n",
    "    print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "w = csv.writer(open('header_results.csv', 'w'))\n",
    "for key, val in results.items():\n",
    "    w.writerow([key,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on dictionary of 3 key/value\n",
    "\n",
    "results = {}\n",
    "for key, value in dict_first3.items():\n",
    "    query =  {\"simple_query_string\": {\"query\": value}}\n",
    "    search1 = ES_search(\"headers_new\", query)\n",
    "    results[key] = list(search1.keys())\n",
    "    time.sleep(3)\n",
    "    es.indices.refresh(index='headers_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0002': ['a4c', 'a6c', 'a5c', 'c3b', 'a3', 'a5b', 'a2', 'e4', 'a7c', 'a7b'],\n",
       " '0003': ['a4c', 'a6c', 'a3', 'a5c', 'e4', 'c3b', 'a5b', 'a2', 'a7c', 'a7b'],\n",
       " '0004': ['a4c', 'a6c', 'a5c', 'a3', 'e4', 'a7c', 'a2', 'a5b', 'c3b', 'e6']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
